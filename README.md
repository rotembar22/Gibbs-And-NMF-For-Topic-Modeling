As part of an NLP course, we review the Gibbs sampling and non-negative matrix factorization models for topic modeling. In joint work with Uria Levkovich, we reviewed the theoretical background of each model, raised several research questions, performed tests, and analyzed the results.
My part in this work focused on the NMF model.


# ABSTRACT
Topic modeling for text classification is an unsupervised machine learning approach for finding abstract topics in an extensive collection of documents. It scans a set of documents, detects word and phrase patterns within them, and automatically clusters word groups and similar expressions that best characterize a set of documents. It is an essential concept of the traditional Natural Language Processing (NLP) approach because of its potential to obtain semantic relationships between words in the document clusters. Gibbs sampling is a method used to approximate the posterior distribution of a parameter of interest. Gibbs and non-negative matrix factorization (NMF) are common frameworks used for models to detect topics. LDA - Gibbs uses a probabilistic approach, whereas NMF uses a matrix factorization approach. This article will describe topic modeling, how Gibbs sampling and NFM models work, and implementation of the models
