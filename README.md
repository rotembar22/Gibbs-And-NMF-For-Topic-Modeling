# Topic Modeling in Natural Language Processing

![Seer Logo](https://www.analyticsvidhya.com/wp-content/uploads/2016/08/Modeling1.png)

## Authors
[:godmode: Bar Rotem](https://github.com/rotembaruch)<br>
[:suspect: Levkovich Uria](https://github.com/uriaLevko)<br>


## Abstract
Topic modeling for text classification is an unsupervised machine learning approach for finding abstract topics in an extensive collection of documents. It scans a set of documents, detects word and phrase patterns within them, and automatically clusters word groups and similar expressions that best characterize a set of documents. It is an essential concept of the traditional Natural Language Processing (NLP) approach because of its potential to obtain semantic relationships between words in the document clusters. Gibbs and non-negative matrix factorization (NMF) are common frameworks for detecting topics. Gibbs uses a probabilistic approach, whereas NMF uses a matrix factorization approach. This article will describe topic modeling, how Gibbs sampling and NFM models work, and implementation of the models.


**Links**:

[**Jupyter notebook**](https://github.com/rotembaruch/Gibbs-And-NMF-For-Topic-Modeling/blob/main/non%20negative%20matrix%20factorization%20for%20topic%20modeling.ipynb "Jupyter notebook")

[**Paper**](https://github.com/rotembaruch/Gibbs-And-NMF-For-Topic-Modeling/blob/main/Gibbs%20Sampling%20And%20Non-Negative%20Matrix%20Factorization%20For%20Topic%20Modeling.pdf "Paper")
